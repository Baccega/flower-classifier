{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flower classifier 2 layers\n",
    "\n",
    "_Author: Baccega Sandro_\n",
    "\n",
    "In this notebook we will classify Oxford's `102 Category Flower Dataset` that can be found [here](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html).\n",
    "\n",
    "Another required asset to run this code is `Oxford-102_Flower_dataset_labels.txt` from JosephKJ that can be found [here](https://gist.github.com/JosephKJ/94c7728ed1a8e0cd87fe6a029769cde1), this external file contains the flowers names.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as skio\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from tabulate import tabulate\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# --- CONSTANTS AND HYPERPARAMETERS ---\n",
    "\n",
    "CHOOSEN_MODEL = 3\n",
    "SEED = 151836\n",
    "DATASET_SPLIT = 0.8             # Get 80% of dataset for training, the rest for testing/validation\n",
    "TESTING_VALIDATION_SPLIT = 0.5  # Get 50% of testing dataset for validation, the rest for testing\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10\n",
    "HIDDEN_LAYERS_OUTPUT_CHANNELS = [8, 16]\n",
    "CROPPED_IMAGES_SIZE = 200\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Assets location\n",
    "\n",
    "RAW_IMAGE_LABELS_MAT_FILE = \"assets/imagelabels.mat\"\n",
    "RAW_DATASET_LABELS_FILE = \"assets/Oxford-102_Flower_dataset_labels.txt\"\n",
    "RAW_DATASET_IMAGES_FOLDER = \"assets/jpg\"\n",
    "RAW_SEGMENTED_IMAGES_FOLDER = \"assets/segmim\"\n",
    "\n",
    "# Data folder location\n",
    "\n",
    "DATASET_IMAGES_FOLDER = \"data/images\"\n",
    "\n",
    "# Setting seed\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set device to use for computations\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = (\n",
    "#     \"cuda\"\n",
    "#     if torch.cuda.is_available()\n",
    "#     else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# )\n",
    "\n",
    "print(f\"-----\\nPyTorch version: {torch.__version__}\\nDevice: {device}\\n-----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the sorted data folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame()\n",
    "metadata['images'] = sorted(os.listdir(RAW_DATASET_IMAGES_FOLDER))\n",
    "metadata[\"labels\"] = loadmat(RAW_IMAGE_LABELS_MAT_FILE)[\"labels\"][0] - 1 \n",
    "metadata['labels'] = metadata['labels'].astype(str)\n",
    "\n",
    "# print(metadata)\n",
    "\n",
    "groups = metadata.groupby(\"labels\")[\"images\"].apply(list)\n",
    "\n",
    "# Creating data folder (ImageFolder dataset)\n",
    "# If data folder exists, do not create images folder\n",
    "# if not os.path.isdir(DATASET_IMAGES_FOLDER):\n",
    "#     print(\"Creating data folder\")\n",
    "#     os.mkdir(DATASET_IMAGES_FOLDER)\n",
    "\n",
    "#     for category, images in groups.items():\n",
    "#         os.mkdir(\"{}/{}\".format(DATASET_IMAGES_FOLDER, category))\n",
    "#         for image in images:\n",
    "#             shutil.copyfile(\n",
    "#                 \"{}/{}\".format(RAW_DATASET_IMAGES_FOLDER, image),\n",
    "#                 \"{}/{}/{}\".format(DATASET_IMAGES_FOLDER, category, image),\n",
    "#             )\n",
    "\n",
    "#     print(\"Done - data folder creation\")\n",
    "# else:\n",
    "#     print(\"Skipping - data folder creation\")\n",
    "\n",
    "\n",
    "# Creating category to label reference\n",
    "rawLabelReferenceData = np.loadtxt(RAW_DATASET_LABELS_FILE, dtype=\"str\", delimiter=\"\\n\")\n",
    "labelReferenceData = list(map(lambda str: str[2:-1], rawLabelReferenceData))\n",
    "\n",
    "i = 0\n",
    "labelReference = {}\n",
    "\n",
    "for category, images in groups.items():\n",
    "    labelReference[category] = labelReferenceData[i]\n",
    "    i += 1\n",
    "\n",
    "print(\"Done - label reference creation\")\n",
    "# print(labelReference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDateset(Dataset):\n",
    "    def __init__(self, metadata, transform=None):\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.metadata.iloc[index, 0]\n",
    "        image = skio.imread(\"{}/{}\".format(RAW_DATASET_IMAGES_FOLDER, image_path))\n",
    "        label = torch.tensor(int(self.metadata.iloc[index, 1]))\n",
    "        # label = F.one_hot(label, num_classes=102)\n",
    "        # label = label.float()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.RandomRotation(45),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize((CROPPED_IMAGES_SIZE, CROPPED_IMAGES_SIZE)),\n",
    "        # transforms.RandomCrop(CROPPED_IMAGES_SIZE),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.AutoAugment(),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = FlowerDateset(metadata, transform=transform)\n",
    "\n",
    "train_set_size = int(len(dataset) * DATASET_SPLIT)\n",
    "validation_set_size = int((len(dataset) - train_set_size)  * TESTING_VALIDATION_SPLIT)\n",
    "test_set_size = len(dataset) - validation_set_size - train_set_size\n",
    "\n",
    "train_dataset, validation_test_dataset = data.random_split(\n",
    "    dataset, [train_set_size, validation_set_size + test_set_size]\n",
    ")\n",
    "validation_dataset, test_dataset = data.random_split(\n",
    "    validation_test_dataset, [validation_set_size, test_set_size]\n",
    ")\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train dataset size: {}\".format(len(train_dataset)))\n",
    "print(\"Validation dataset size: {}\".format(len(validation_dataset)))\n",
    "print(\"Test dataset size: {}\".format(len(test_dataset)))\n",
    "\n",
    "n_rows = 2\n",
    "n_cols = 4\n",
    "\n",
    "for x, y in train_dataloader:\n",
    "    x = x.to(device)\n",
    "    fig, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(12, 8))\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            ax[i, j].imshow(x[(i * n_cols) + j].cpu().permute(1,2,0))\n",
    "            ax[i, j].axis(\"off\")\n",
    "            #ax[i, j].set_title(labelReference[label])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "Here we create our CNN model with 2 simple layers or use the ResNet18 model.\n",
    "\n",
    "We use a kernel_size=(3,3), stride=(1,1) and padding=(1,1) in order to have a same convolution (manteins image size).\n",
    "\n",
    "We use a MaxPool2d to reduce the size of the network by half efficiently.\n",
    "\n",
    "We use a Linear to create the last fully connected layer.  \n",
    "\n",
    "We use the Adam optimizer and the CrossEntropyLoss as our loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My2LayerCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_in_channels=3,\n",
    "        num_out_channels=HIDDEN_LAYERS_OUTPUT_CHANNELS,\n",
    "        img_width=CROPPED_IMAGES_SIZE,\n",
    "        img_height=CROPPED_IMAGES_SIZE,\n",
    "        num_classes=102,\n",
    "    ):\n",
    "        super(My2LayerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=num_in_channels,\n",
    "            out_channels=num_out_channels[0],\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=num_out_channels[0],\n",
    "            out_channels=num_out_channels[1],\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=int(img_width / 4) * int(img_height / 4) * num_out_channels[1],\n",
    "            out_features=num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x.reshape(x.shape[0], -1))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Load saved model\n",
    "if CHOOSEN_MODEL == 0:\n",
    "    model = My2LayerCNN()\n",
    "    model.load_state_dict(torch.load('model.pt'))\n",
    "    model.eval()\n",
    "# My 2 Layer CNN model\n",
    "if CHOOSEN_MODEL == 1:\n",
    "    model = My2LayerCNN()\n",
    "# Resnet\n",
    "if CHOOSEN_MODEL == 2:\n",
    "    model = models.resnet18()\n",
    "    model.fc = nn.Linear(model.fc.in_features, 102)\n",
    "# Resnet pretrained\n",
    "if CHOOSEN_MODEL == 3:\n",
    "    model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 102)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results_data = pd.DataFrame(\n",
    "    {\"Epoch\": [], \"Predictions\": [], \"Samples\": [], \"Accuracy\": [], \"Loss\": []}\n",
    ")\n",
    "\n",
    "def check_accuracy(data_loader, model):\n",
    "    n_corrects = 0\n",
    "    n_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            # Sending data to device\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Forward propagation\n",
    "            y_hat = model(x)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predictions = y_hat.max(1)\n",
    "            n_corrects += (predictions == y).sum()\n",
    "            n_samples += predictions.size(0)\n",
    "\n",
    "        perc = (n_corrects.item() / n_samples) * 100\n",
    "        return (n_corrects.item(), n_samples, perc)\n",
    "\n",
    "\n",
    "if CHOOSEN_MODEL != 0: \n",
    "    model.train()\n",
    "    # Tensorboard writer\n",
    "    writer = SummaryWriter()\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        running_loss = 0\n",
    "        with tqdm.tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "\n",
    "            for index, (x, y) in enumerate(tepoch):\n",
    "                # Send the data to the device\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # Forward propagation\n",
    "                y_hat = model(x)\n",
    "                loss = criterion(y_hat, y)\n",
    "                running_loss += loss\n",
    "\n",
    "                # Backward propagation\n",
    "                optimizer.zero_grad()\n",
    "                # Calculate derivatives for every parameters\n",
    "                loss.backward()\n",
    "                # Do gradient descent\n",
    "                optimizer.step()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, loss: {running_loss}\")\n",
    "            n_corrects, n_samples, accuracy = check_accuracy(validation_dataloader, model)\n",
    "            print(f\"Accuracy: {n_corrects}/{n_samples} = {accuracy:.2f}%\")\n",
    "            # writer.add_figure(\"Confusion matrix\", createConfusionMatrix(validation_dataloader, model), epoch)\n",
    "            training_results_data.loc[len(training_results_data.index)] = [\n",
    "                int(epoch + 1),\n",
    "                int(n_corrects),\n",
    "                int(n_samples),\n",
    "                accuracy,\n",
    "                running_loss.item(),\n",
    "            ]\n",
    "            writer.add_scalar(\"Loss/train\", running_loss, epoch)\n",
    "            writer.add_scalar('Accuracy/validation', accuracy, epoch)\n",
    "\n",
    "\n",
    "    writer.flush()\n",
    "    \n",
    "\n",
    "    # Saving model\n",
    "    torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "# Saving parameters\n",
    "with open('model_parameters.txt', 'w') as file:\n",
    "    file.write(f\"CHOOSEN_MODEL = {CHOOSEN_MODEL}\\n\")\n",
    "    file.write(f\"SEED = {SEED}\\n\")\n",
    "    file.write(f\"DATASET_SPLIT = {DATASET_SPLIT}\\n\")\n",
    "    file.write(f\"TESTING_VALIDATION_SPLIT = {TESTING_VALIDATION_SPLIT}\\n\")\n",
    "    file.write(f\"BATCH_SIZE = {BATCH_SIZE}\\n\")\n",
    "    file.write(f\"N_EPOCHS = {N_EPOCHS}\\n\")\n",
    "    file.write(f\"HIDDEN_LAYERS_OUTPUT_CHANNELS = {HIDDEN_LAYERS_OUTPUT_CHANNELS}\\n\")\n",
    "    file.write(f\"CROPPED_IMAGES_SIZE = {CROPPED_IMAGES_SIZE}\\n\")\n",
    "    file.write(f\"LEARNING_RATE = {LEARNING_RATE}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createConfusionMatrix(data_loader, model):\n",
    "    confusion_matrix = torch.zeros((102, 102))\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, classes) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            classes = classes.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    fig = plt.figure(figsize=(250,250))\n",
    "\n",
    "    # confusion_matrix_np = confusion_matrix.numpy()\n",
    "    # np.savetxt(\"confusion_matrix.csv\", confusion_matrix_np, delimiter=\",\")\n",
    "    \n",
    "    class_names = list(labelReference.values())\n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
    "    df_cm.to_csv('confusion_matrix.csv')\n",
    "\n",
    "    heatmap = sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=10)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=10)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig\n",
    "# def createConfusionMatrix(data_loader, model):\n",
    "#     confusion_matrix = np.zeros((102, 102))\n",
    "#     with torch.no_grad():\n",
    "#         for i, (inputs, classes) in enumerate(data_loader):\n",
    "#             inputs = inputs.to(device)\n",
    "#             classes = classes.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             print(preds)\n",
    "#             for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "#                     confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "#     # fig = plt.figure(figsize=(250,250))\n",
    "\n",
    "#     class_names = list(labelReference.values())\n",
    "#     df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
    "#     df_cm.to_csv('confusion_matrix.csv')\n",
    "#     # heatmap = sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "#     # heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=10)\n",
    "#     # heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=10)\n",
    "#     # plt.ylabel('True label')\n",
    "#     # plt.xlabel('Predicted label')\n",
    "#     # return fig\n",
    "\n",
    "if CHOOSEN_MODEL != 0:\n",
    "    print(\"Training/Validation results\")\n",
    "    formatted_training_results_data = pd.DataFrame(\n",
    "        {\"Epoch\": [], \"Predictions/Samples\": [], \"Accuracy\": [], \"Loss\": []}\n",
    "    )\n",
    "    for index, row in training_results_data.iterrows():\n",
    "        formatted_training_results_data.loc[len(formatted_training_results_data.index)] = [\n",
    "                row[0],\n",
    "                f\"{int(row[1])}/{int(row[2])}\",\n",
    "                f\"{row[3]:.2f}%\",\n",
    "                f\"{row[4]}\",\n",
    "            ]\n",
    "\n",
    "    print(tabulate(formatted_training_results_data, headers = 'keys', tablefmt = 'fancy_grid', showindex=\"never\"))\n",
    "\n",
    "    plt.plot(training_results_data[\"Epoch\"], training_results_data[\"Accuracy\"])\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.yscale('log')\n",
    "    # plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(training_results_data[\"Epoch\"], training_results_data[\"Loss\"])\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    # plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Testing results\")\n",
    "n_corrects, n_samples, accuracy = check_accuracy(test_dataloader, model)\n",
    "print(f\"Testing accuracy: {n_corrects}/{n_samples} ({accuracy:.2f}%)\")\n",
    "writer.add_figure(\"Confusion matrix\", createConfusionMatrix(test_dataloader, model))\n",
    "writer.flush()\n",
    "writer.close()\n",
    "# createConfusionMatrix(test_dataloader, model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a39c5b69f984fae0f6e851bad95ae9e9b17b3f6ad94bf0d0d6a3a57fb058815b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
