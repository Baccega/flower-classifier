{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flower classifier \n",
    "\n",
    "*Author: Baccega Sandro*\n",
    "\n",
    "In this notebook we will classify Oxford's `102 Category Flower Dataset` that can be found [here](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html).\n",
    "\n",
    "Another required asset to run this code is `Oxford-102_Flower_dataset_labels.txt` from JosephKJ that can be found [here](https://gist.github.com/JosephKJ/94c7728ed1a8e0cd87fe6a029769cde1), this external file contains the flowers names.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "PyTorch version: 1.13.0.dev20220608\n",
      "Device: mps\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# --- CONSTANTS AND HYPERPARAMETERS ---\n",
    "\n",
    "SEED = 151836\n",
    "CROPPED_IMAGES_SIZE = 224\n",
    "\n",
    "DATASET_SPLIT = 0.8         # Get 80% of dataset for training, the rest for validating \n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 25\n",
    "\n",
    "# Assets location\n",
    "\n",
    "RAW_IMAGE_LABELS_MAT_FILE = \"assets/imagelabels.mat\"\n",
    "RAW_DATASET_LABELS_FILE = \"assets/Oxford-102_Flower_dataset_labels.txt\"\n",
    "RAW_DATASET_IMAGES_FOLDER = \"assets/jpg\"\n",
    "RAW_SEGMENTED_IMAGES_FOLDER = \"assets/segmim\"\n",
    "\n",
    "# Data folder location\n",
    "\n",
    "DATASET_IMAGES_FOLDER = \"data/images\"\n",
    "\n",
    "# Setting seed\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set device to use for computations\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "print(f\"-----\\nPyTorch version: {torch.__version__}\\nDevice: {device}\\n-----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the sorted data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping - data folder creation\n",
      "Done - label reference creation\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Image'] = sorted(os.listdir(RAW_DATASET_IMAGES_FOLDER))\n",
    "df['Category'] = loadmat(RAW_IMAGE_LABELS_MAT_FILE)['labels'][0] - 1\n",
    "# df['Category'] = df['Category'].astype(str)\n",
    "\n",
    "groups = df.groupby('Category')['Image'].apply(list)\n",
    "\n",
    "# If data folder exists, do not create images folder\n",
    "if not os.path.isdir(DATASET_IMAGES_FOLDER):\n",
    "    print(\"Creating data folder\")        \n",
    "    os.mkdir(DATASET_IMAGES_FOLDER)\n",
    "    \n",
    "    for category, images in groups.items():\n",
    "        os.mkdir('{}/{}'.format(DATASET_IMAGES_FOLDER, category))\n",
    "        for image in images:\n",
    "            shutil.copyfile('{}/{}'.format(RAW_DATASET_IMAGES_FOLDER, image), '{}/{}/{}'.format(DATASET_IMAGES_FOLDER,category,image))\n",
    "\n",
    "    print(\"Done - data folder creation\")\n",
    "else:\n",
    "    print(\"Skipping - data folder creation\")        \n",
    "\n",
    "\n",
    "# Creating category to label reference\n",
    "rawLabelReferenceData = np.loadtxt(RAW_DATASET_LABELS_FILE,dtype=\"str\", delimiter='\\n')\n",
    "labelReferenceData = list(map(lambda str: str[2:-1], rawLabelReferenceData))\n",
    "\n",
    "i = 0\n",
    "labelReference = {}\n",
    "\n",
    "for category, images in groups.items():\n",
    "    labelReference[category] = labelReferenceData[i]\n",
    "    i += 1\n",
    "\n",
    "print(\"Done - label reference creation\")\n",
    "# print(labelReference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 6551\n",
      "Validation dataset size: 1638\n"
     ]
    }
   ],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(DATASET_IMAGES_FOLDER)\n",
    "\n",
    "train_set_size = int(len(dataset) * DATASET_SPLIT)\n",
    "valid_set_size = len(dataset) - train_set_size\n",
    "\n",
    "untransformed_train_dataset, untransformed_validation_dataset = data.random_split(dataset, [train_set_size, valid_set_size])\n",
    "\n",
    "train_dataset = TransformDataset(\n",
    "    untransformed_train_dataset, transform=transforms.Compose([\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomResizedCrop(CROPPED_IMAGES_SIZE),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")\n",
    "validation_dataset = TransformDataset(\n",
    "    untransformed_validation_dataset, transform=transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(CROPPED_IMAGES_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"Train dataset size: {}\".format(len(train_dataset)))\n",
    "print(\"Validation dataset size: {}\".format(len(validation_dataset)))\n",
    "# TODO: print transformed images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "Here we create our CNN model with 2 simple layers.\n",
    "\n",
    "We use a kernel_size=(3,3), stride=(1,1) and padding=(1,1) in order to have a same convolution.\n",
    "\n",
    "We use a MaxPool2d to reduce the size of the network by half efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 102])\n"
     ]
    }
   ],
   "source": [
    "class MyCNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_in_channels=3,\n",
    "        num_out_channels=[8, 16],\n",
    "        img_width=CROPPED_IMAGES_SIZE,\n",
    "        img_height=CROPPED_IMAGES_SIZE,\n",
    "        num_classes=102,\n",
    "    ):\n",
    "        super(MyCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=num_in_channels,\n",
    "            out_channels=num_out_channels[0],\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=num_out_channels[0],\n",
    "            out_channels=num_out_channels[1],\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=int(img_width / 4) * int(img_height / 4) * num_out_channels[1],\n",
    "            out_features=num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x.reshape(x.shape[0], -1))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyCNNModel()\n",
    "x = torch.randn(32, 3, CROPPED_IMAGES_SIZE, CROPPED_IMAGES_SIZE)\n",
    "y = model(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a39c5b69f984fae0f6e851bad95ae9e9b17b3f6ad94bf0d0d6a3a57fb058815b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
